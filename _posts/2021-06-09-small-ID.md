---
layout: post
title: The small-ID technique (1)
redirect_from: "/2021/06/09/small-ID-1/"
permalink: small-ID-1
---

In this series of posts, I'll describe a rather recent, very neat and 
powerful technique for the LOCAL model. 
I will call it the *small-ID technique*, but it has several names.
It can be called the *speed-up technique*, because it is an automatic way
to speed-up an algorithm whose complexity is in a given range.
It can also be called the *simulation technique* because the nodes simulate
computation in another graph.
And it is also a *gap technique*, because it is used to prove that there
is no problem whose complexity is in some range.

I chose to use yet another name to avoid confusion with the round elimination
technique, which is completely different, but used to be called simulation
technique (e.g. in the
[series of posts](https://discrete-notes.github.io/simulation-1) I wrote about
round elimination).

This technique appeared for the first time in the celebrated paper
[An Exponential Separation between Randomized and Deterministic Complexity in the LOCAL Model](https://epubs.siam.org/doi/10.1137/17M1117537)
by [Yi-Jun Chang](https://sites.google.com/a/umich.edu/yi-jun-chang/),
[Tsvi Kopelowitz](https://sites.google.com/site/kopelot/), and
[Seth Pettie](https://web.eecs.umich.edu/~pettie/).

This first post will be about the idea of the technique.


## Setting 
The technique works for locally checkable labelings (LCL).
These are the distributed problems whose outputs can be checked locally.
For example, a proper coloring can be checked locally:
one only needs to check every ball of radius 1 in the graph to tell whether 
a coloring is proper or not.
Many other problems fall into this category, including maximal matching, 
maximal independent set and sinkless orientation.
Here, we will consider an arbitrary LCL, and its verification radius, that 
is the distance up to which a nodes needs to look to check that the 
solution looks good locally, is some constant that we will call $r$.

We consider networks with bounded maximum degree $\Delta$, and assume that 
the nodes are given unique identifiers on $O(\log n)$ bits, or equivalently
unique identifiers in $\{1,...,n^2\}$.

For such LCL problems in such graphs, there exist algorithms with 
complexities such as $\Theta(1)$, $\Theta(\log^*n)$ or $\Theta(\log n)$, 
where $n$ is the number of nodes in the network. 

 
## First attempt

The core idea of the technique is that if the nodes believe they live in a
smaller graph they will compute faster. Let's try something along these 
lines. 

For concreteness, consider a coloring algorithm $A$ that runs in $f(n)$ 
rounds, where $f$ is neither constant, nor linear in $n$, say $\sqrt(n)$.
Consider a  huge graph $H$ on $h=1000000$ vertices. 
Every node will output a color after $f(h)=f(1000000)$ rounds, and 
globally the coloring computed is a proper coloring.
If we zoom on some given node $u$, we can see that it stops after 
$f(1000000)$ rounds, outputs a color, and that its neighbors also stop in 
that time, also output a color, and these colors are different from u. 
Now, suppose I take a small subgraph $S$ of $H$ on $s=100$ vertices, 
containing $u$ and its neighbors.
As the size of $S$ is $s$, if I run my $f(n)$-algorithm on $S$, it should
output after $f(s)=f(100)$ rounds. In particular, $u$ stops after $f(100)$ 
rounds, outputs a color, and its neighbors do the same with a different color. 
Note that $s << h$, thus $f(s) << f(h)$ because $f$ is not constant.
Thus to do the same job, which is output a coloring that
is locally correct around $u$, we took much less time!

Now, we can design a new algorithm $A'$.
First, it gathers its neighborhood at some distance $d$, such that the subgraph
induced by this neighborhood has at least $s=100$ vertices, and radius at least
f(100). Then it runs $A$ on this subgraph which takes $f(100)$ rounds, and
output the same color as $A$. 
This algorithm is much faster than $A$. 
And it is correct: for every node $u$, the colors of its neighbors are 
different from its own so globally the coloring in the large graph is correct. 

This is too good to be true, so where's the catch? The problem is the 
identifier range. Our algorithm $A$ worked with the assumption that the 
identifiers where in the range $\{1,...,n^2\}$ where $n$ is the size of the 
network. Now by taking a subgraph of size $s$, to be guaranteed that $A$ 
will output a proper coloring after $f(s)$ steps, we need to have 
identifiers in $\{1,...,s^2\}$, and we actually have identifier in 
$\{1,...,h^2\}$, where $h$ is the size of the real graph. 

## Second attempt 

To make our idea work, what we will do is to compute new identifiers, that
are small enough to ensure that the algorithm computing on the subgraph 
will indeed output a correct solution, in less time. 
Hence the name of small-ID technique. 
Again there is a problem: if we use new identifiers on $o(\log n)$, then 
these cannot be unique in the graph, thus they are not unique identifiers. 
The idea is that we will make sure that two identifiers that are equal are 
far enough in the graph, so that the algorithm that believes that it lives 
in a graph of size $s$, will look at distance $f(s)$, and will not see two 
identical identifiers. If this holds then the output of $A$ should be 
locally correct, and then globally correct, because we tackle only LCLs. 

In the next post, I will describe how the technique works on paths. We'll 
see that there are some subtleties with the way we compute the small-IDs, 
but the main idea is just that.



 
