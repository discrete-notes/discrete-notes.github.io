<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Discrete notes</title>
    <description>Blog</description>
    <link>https://discrete-notes.github.io/</link>
    <atom:link href="https://discrete-notes.github.io//feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Network decomposition 5&amp;#58; The algorithm</title>
        <description>&lt;p&gt;This is the fifth post of a series on distributed network decomposition. 
The introductory post of this series is 
&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post, we (finally!) describe the algorithm of the paper of Ghaffari and Rozhon.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/caravane-6.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;recap-of-previous-episodes&quot;&gt;Recap of previous episodes&lt;/h3&gt;

&lt;p&gt;In the previous posts, we saw what a network decomposition is, why it is useful,
how to compute it in a sequential model via ball-carving,
and why computing only a weak decomposition is not problematic. In this post we 
show the new algorithm to build a (weak) network decomposition.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;An overview of the algorithm can be given	 as a comparison with the centralized
construction presented in
&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-3-centralized&quot;&gt;the third post&lt;/a&gt;
of this series.
First, just as in the centralized construction, the color classes will be built 
one by one, and each color will “take” a constant fraction of the remaining nodes,
which insures that we get a logarithmic number of colors.
As a consequence, in the remaining of this post, we will be interested only in
building the first color class, as the other classes are built the same way.
More precisely, as in the centralized construction, our goal is now to build a 
set of clusters separated by frozen nodes, with small (weak) diameters, and
with the constraint that not too many nodes are frozen.&lt;/p&gt;

&lt;p&gt;Now, unlike in the sequential construction, we cannot grow balls one after the
other (this would take at least diameter time, and the diameter can be large).
One could try to first find a good set of nodes, far away one from the others,
and to start growing balls in parallel from these nodes. But finding this set of
nodes looks as hard 
as computing the decomposition itself (remember that we build a network
decomposition to compute maximal independent sets, among other things).&lt;/p&gt;

&lt;p&gt;Hence another technique has to be used.&lt;/p&gt;

&lt;h2 id=&quot;separating-the-graph&quot;&gt;Separating the graph&lt;/h2&gt;

&lt;p&gt;Instead of growing clusters, we will separate the graph into smaller and smaller 
pieces (by freezing nodes).&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/ND-separating-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/ND-separating-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/ND-separating-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A key idea of the algorithm is to use the digits of the identifiers of the
nodes written in binary. A first attempt in this direction is the following. Let
us start by having two sets of nodes, the ones whose IDs begin with 0, and
the ones whose IDs begin with 1. We will draw these sets as respectively
blue and red. Now if we want to be sure that not two nodes of different colors
(red and blue) are adjacent, we can freeze one of the nodes for every
bicolored edge. In a good scenario, we get the following picture.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/ND-red-blue-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;70%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/ND-red-blue-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;70%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;But this is a good case, and in general this technique is bad. First, of
course as the identifiers are not required to be uniformly distributed between
red and blue, all nodes could be blue. But this is not really a problem: anyway
we will have to somehow iterate the construction to all digits one by one, and the
IDs are all distinct, thus at some point we will have two non-empty groups.
The main problem is the number of frozen nodes: it can be huge, almost all
the nodes can be frozen. This is bad for us: we want to get a
color class that contains a constant fraction of the nodes of the graph. (Note
that even if you could get only half of the nodes frozen at this step, as we
will iterate this for all digits, it wouldn’t work.)&lt;/p&gt;

&lt;p&gt;So now we have to answer the question: how to get a small number of frozen nodes
when separating the red and the blue nodes?&lt;/p&gt;

&lt;p&gt;Before going into that, let us restate at which level of the explanation we
are (as there are quite a lot of levels).
We want to build a network decomposition, for this we
first want to build a first color class, and for this we first want to separate
the nodes whose identifiers start with 0 (the blue nodes in the pictures) and
the nodes whose identifiers start with 1 (the red nodes in the pictures)&lt;/p&gt;

&lt;h2 id=&quot;ball-carving-is-back&quot;&gt;Ball carving is back&lt;/h2&gt;

&lt;p&gt;We know a technique to separate some parts of the graph without freeing too many
nodes: growing a ball like in the third post (that is, “ball carving”). The idea
is to consider the blue and red nodes in a non-symmetrical way: blue nodes are 
starting point for ball carving, and red nodes want to join such balls. That is, 
every blue node starts as a one-node ball, and then, until this is not possible:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the (non-frozen) red nodes that have at least one blue neighbor propose to
join the ball of one of these blue nodes (arbitrarily),&lt;/li&gt;
  &lt;li&gt;the blue balls accept if the number of red nodes proposing is large enough, and
reject otherwise,&lt;/li&gt;
  &lt;li&gt;if the blue ball accepts, the red nodes proposing become blue, if it rejects, 
they freeze.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When a red node joins a blue ball, we add an edge to the Steiner tree of this
ball to link this node. Also this node change its ID, to take the identifier of
the ball. Hence these numbers are not really identifiers any more, and from now on
we’ll say &lt;em&gt;label&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;After this ball carving process we have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;more blue nodes and less red nodes,&lt;/li&gt;
  &lt;li&gt;no edge between a red and a blue node,&lt;/li&gt;
  &lt;li&gt;not too many frozen nodes,&lt;/li&gt;
  &lt;li&gt;clusters whose Steiner tree have at most logarithmic diameter.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Which is pretty good! But of course, the clusters of same color can be adjacent,
and we don’t want that. We have to refine this partition. Note that now the
blue and red nodes will live in two independent worlds, separated by frozen nodes.&lt;/p&gt;

&lt;h2 id=&quot;the-second-digit&quot;&gt;The second digit&lt;/h2&gt;

&lt;p&gt;Now we will go to the second digit of the labels. There are quite a few things
that are simpler for the first digit, thus we cannot simply say “do it again”.&lt;/p&gt;

&lt;p&gt;First of
all forget about the colors of the nodes given by the first digit, and color
them with the color of the second digit (blue for _0 and red for _1).
This means that we have again adjacent red and blue clusters: either two clusters that
were previously blue whose labels differ on the second bit, or
two nodes that were previously red (and are also clusters restricted to one node),
also with different second digit.&lt;/p&gt;

&lt;p&gt;A natural way to iterate the construction would be the following: consider the
clusters as the nodes of the previous phase and make the same construction at
this level. That is, the red clusters propose to the blue clusters etc.
But this would not be good: we could freeze an arbitrary red cluster because it
proposed to blue clusters with few neighbors, and this red cluster might be huge.&lt;/p&gt;

&lt;p&gt;What we do instead is to make the red nodes act individually without
coordination inside the red cluster (but keep the blue nodes acting as a cluster).
That is: every red node that is adjacent to a blue cluster proposes to join this 
cluster (breaking ties arbitrarily), and is accepted if enough such nodes want 
to join. 
And we do the ball carving this way.
A red node, if it joins a blue cluster, will be linked the Steiner tree of this
cluster, and take the label of this cluster.&lt;/p&gt;

&lt;h2 id=&quot;a-few-non-intuitive-things&quot;&gt;A few non-intuitive things&lt;/h2&gt;

&lt;p&gt;Now, there are a few non-intuitive things going on. 
First, suppose that after the first digit, we had a nice
cluster, and that the second digit of its label is a 1. Then, this cluster is
basically dismantled during the phase corresponding to the second digit, because 
many nodes of this cluster might be eaten by adjacent blue clusters. This 
looks counter-intuitive but seems necessary to separate clusters further.&lt;/p&gt;

&lt;p&gt;Second, it is possible that
nodes having the same label, do not form a connected component anymore. Indeed,
consider a cluster that was blue before, and now is red. With the ball-carving,
some nodes of this
cluster become blue, some nodes are frozen and some nodes are still red and
separated from blue nodes by frozen nodes. It is possible that these red nodes
(that have the same labels) form disconnected islands that were linked
by nodes that are now blue or frozen. But they still form a cluster: we have
not erased the previous Steiner tree, thus they are still linked. And this is
important: at the next digit, this label may give color blue, and then these
nodes have to act in a coordinated way. This is why we need to talk about weak
decompositions and not strong ones.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/ND-non-intuitive-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/ND-non-intuitive-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/ND-non-intuitive-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;One more non-intuitive thing is that, because of the same reason, the nodes of a
cluster may have a label that is the ID of a node that is not anymore in the
cluster.&lt;/p&gt;

&lt;h2 id=&quot;thats-it&quot;&gt;That’s it!&lt;/h2&gt;

&lt;p&gt;This is pretty much the algorithm! Then you just iterate on the digits of the
labels, and at the end, the all clusters are separated by frozen nodes, have
a controlled weak diameter, and not too many nodes have been frozen. This way
we get the first color class, and then we can go to second color class,etc.&lt;/p&gt;

&lt;p&gt;By setting the right parameters for the ball carving, you get an algorithm that
use polylogarithmic time to build a network decomposition with logarithmic weak
diameter and a logarithmic number of colors.&lt;/p&gt;

</description>
        <pubDate>Thu, 21 May 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///network-decomposition-5-algorithm</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///network-decomposition-5-algorithm</guid>
      </item>
    
      <item>
        <title>Notes for (rainy) May</title>
        <description>&lt;p&gt;Notes for May.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/caravane-nuit.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;95%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;active-learning-and-interactive-protocols&quot;&gt;Active learning and interactive protocols&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Grace_Murray_Hopper_Award&quot;&gt;Grace Murray Hopper Award&lt;/a&gt;
has been given to &lt;a href=&quot;http://www.cs.cmu.edu/~ninamf/&quot;&gt;Maria Florina “Nina” Balcan&lt;/a&gt; 
for her work in machine learning. One of her contributions is in
&lt;em&gt;&lt;a href=&quot;Active learning (machine learning)&quot;&gt;active machine learning&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In a classic setting of machine learning, the machine is given a lot of 
labeled data and must produce a classifier: an algorithm that can guess the label
of a new data point. In active learning, this is made interactive: the machine 
can create a data point and ask for the label of this point. For example in the
following picture, in order to have a good classifier based on the red and blue
points, the machine may ask about the yellow points, to know which hypothesis is 
the best between the green and the pink one.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/active-learning.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A reason to do that is to save time: after processing a fraction of the data, 
the machine can have some preliminary idea of what the classification looks like, 
and only relevant data points at the boundary of this partition are really useful.
Another reason is if the data points themselves are costly. I remember a talk&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
where the goal was to decide whether small islands close to the shore reduce 
or increase the damages of a tsunami. If you have perfect knowledge of the 
dimensions of your island, you can run a precise fluid simulation, and that’s it. 
But if you 
want a big picture of whether islands in general are bad or good, then you would 
need to run many simulations, and you cannot because it takes too much time. 
Then you can use machine learning to get an approximation of the damage as a 
function of parameters of the island. But remember that any new point takes ages 
to be computed, thus you would like to have only useful data points in you 
learning set. And somehow you cannot always select your points a priori ; then 
having active learning is great.&lt;/p&gt;

&lt;p&gt;Balcan basically proved that active learning can be efficient even in the 
presence of (all kinds of) noise. The Hopper award laudatio cites two other
key works, in semi-supervised learning and clustering.&lt;/p&gt;

&lt;p&gt;[I learned about this in a 
&lt;a href=&quot;https://rjlipton.wordpress.com/2020/04/10/nina-balcan-wins/&quot;&gt;post on Lipton’s blog&lt;/a&gt;, 
that contains the following amusing sentence: “Active learning follows a classic idea 
in computer theory: Making a protocol interactive can often decrease the cost, 
and almost always makes the protocol more complex to understand”.]&lt;/p&gt;

&lt;h2 id=&quot;sprouts-game&quot;&gt;Sprouts game&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://images.math.cnrs.fr&quot;&gt;Images des maths&lt;/a&gt; a maths popularization website in 
French has 
&lt;a href=&quot;http://images.math.cnrs.fr/Savez-vous-planter-les-choux.html&quot;&gt;an article&lt;/a&gt; 
on
&lt;a href=&quot;https://en.wikipedia.org/wiki/Sprouts_(game)&quot;&gt;sprouts&lt;/a&gt;, a combinatorial game 
related to planar graphs.&lt;/p&gt;

&lt;p&gt;It is a paper and pencil game between two players. They start from an arbitrary 
set of points, and then play one after the other. At her turn, the player has to 
do two things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;draw a line between two points (including self-loops)&lt;/li&gt;
  &lt;li&gt;draw a point on this line (not at one of the end points)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and there are a few constraints:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the lines should not intersect&lt;/li&gt;
  &lt;li&gt;every point has degree at most three.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At some point one the players cannot play anymore, this player looses. On the 
following picture, there are three initial points. The blue player makes the 
sixth move, and after that the red player cannot play. Therefore the blue player
wins.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/sprouts-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/sprouts-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/sprouts-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/sprouts-4.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Researchers have completely characterized what are the winning strategies for 
this game up to 44 initial points. And there is the following conjecture: if 
both players play perfectly, the first player cannot win if and only if the 
number of initial points is 0, 1 or 2 modulo 6…!&lt;/p&gt;

&lt;h2 id=&quot;diffraction-and-algorithms&quot;&gt;Diffraction and algorithms&lt;/h2&gt;

&lt;p&gt;The other day, there was a paper on the arxiv named 
&lt;a href=&quot;https://arxiv.org/pdf/2004.07659.pdf&quot;&gt;Algorithmic Foundations for the Diffraction Limit&lt;/a&gt; 
and I thought that “diffraction” had a meaning that I 
didn’t know about, outside of physics. So I looked 
at the abstract, and actually it’s about the good old 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Diffraction&quot;&gt;optical diffraction&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;From what I understood (and remember) here is the topic of the paper. When you 
have two sources of light far away one from the other, and you observe the 
diffraction image created by a circular aperture, you can say “yes there are two 
sources of light, and they are in that position”. But when the two sources are 
close one from the other it’s less clear. It is believed that there is a 
fundamental barrier after which you cannot decide between one and two sources. 
There are several criteria to decide where this barrier is, with various 
explanations but all are heuristics.&lt;/p&gt;

&lt;p&gt;The approach of the paper is to consider that one receives the diffracted image 
through (noisy) samples, and based on this samples must decide whether there are one or 
two sources. This becomes a more statistical problem, and when we ask how many 
samples are needed, then it becomes more algorithmic too. Then one can design 
algorithms with a complexity depending on the number of samples, the noise etc.&lt;/p&gt;

&lt;h2 id=&quot;trusses&quot;&gt;Trusses&lt;/h2&gt;

&lt;p&gt;In the study of social networks, a classic topic is to understand and detect 
communities. In terms of graphs, the simplest object representing a community is 
a clique. But this is a very demanding definition: there are many groups that 
one would call a community, where not everybody knows everybody. A classic 
alternative here is a $k$-core. A $k$-core is a maximal connected subgraph, with all 
nodes having degree at least $k$ in this subgraph. That is everybody should know
at least $k$ other members.&lt;/p&gt;

&lt;p&gt;I learned recently about another such object: $k$-trusses. A $k$-truss is a 
maximal connected subgraph, such that inside this subgraph, every node is 
adjacent to at least $k$ triangles. So now you want that every person can cite 
$k$ pairs of acquaintances who know each other. It’s a more restricted notion 
than $k$-cores, because in some sense the communities must be more “local”, with 
each neighborhood being more connected.&lt;/p&gt;

&lt;p&gt;[I learned about this while taking a look at the papers published in JGAA 
journal for the &lt;a href=&quot;https://discrete-notes.github.io/journal-fees&quot;&gt;post about journal fees&lt;/a&gt;. 
The paper in particular is 
&lt;a href=&quot;http://www.jgaa.info/accepted/2020/527.pdf&quot;&gt;this one&lt;/a&gt;, which discusses bounds 
and algorithms for $k$-trusses.]&lt;/p&gt;

&lt;h2 id=&quot;eggs-and-reconfigurations&quot;&gt;Eggs and reconfigurations&lt;/h2&gt;
&lt;p&gt;The Aperiodical has an 
&lt;a href=&quot;https://aperiodical.com/2020/04/the-big-lock-down-math-off-match-5/&quot;&gt;article&lt;/a&gt; 
about a maths game, with a box of eggs.
The game is the following. You start with a box of eggs in this configuration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/boite-oeufs-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and you should transform it into that configuration:
&lt;img src=&quot;assets/boite-oeufs-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;using only movements of the following form (move one right):
&lt;img src=&quot;assets/boite-oeufs-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The question is what is the minimum number of moves? 
In general I would consider this as just one more maths game, but as I will be 
working on reconfiguration problems next year (in Lyon with 
&lt;a href=&quot;https://pagesperso.g-scop.grenoble-inp.fr/~bousquen/&quot;&gt;Nicolas Bousquet&lt;/a&gt;), 
this is relevant. Reconfiguration problems are problems of the form: how to get 
from configuration A to B in the minimum number of moves of some sort. This is 
important for example for random generation of solutions to combinatorial 
problems.&lt;/p&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Simons institute launches a new series of video, “Theory shorts”. The 
&lt;a href=&quot;https://www.youtube.com/watch?time_continue=14&amp;amp;v=PLAZyu73tWE&amp;amp;feature=emb_logo&quot;&gt;first one&lt;/a&gt; 
is about vision and brain ; quite interesting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An interesting blog about algorithmic fairness that I didn’t know: 
&lt;a href=&quot;https://algorithmicfairness.wordpress.com/category/algorithms/&quot;&gt;Algorithmic fairness&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In &lt;a href=&quot;https://blog.computationalcomplexity.org/2020/05/vidcast-on-conferences.html&quot;&gt;a “vidcast”&lt;/a&gt;, 
Gasarsh and Fortnow talk about conferences. In particular, 
about having conferences sometimes in person, sometimes virtual (and also about 
regional conferences). Incidentially, they mention the fact that the conference 
accronym &lt;em&gt;EC&lt;/em&gt; used to be for &lt;em&gt;Electronic Commerce&lt;/em&gt;, which is weird… (now it’s 
&lt;em&gt;Economy and Computation&lt;/em&gt; which makes more sense).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I realized that the blog was hard to read on smartphone because of the format 
of the pictures, it should be better now.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I started making pictures with my tablet, and it’s a very good solution. Before 
looking into it, I didn’t know one could have a simple software with layers and 
opacity parameters on tablets. 
It’s especially nice to have a first sketch, and then make a precise drawing. 
I use it not only for the blog but also for picture prototypes in manuscripts.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;The talk was a bachelor project defence, in &lt;a href=&quot;http://cmla.ens-paris-saclay.fr/&quot;&gt;CMLA&lt;/a&gt;, an applied maths lab  near Paris.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Mon, 11 May 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///notes-rainy-may-2020</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///notes-rainy-may-2020</guid>
      </item>
    
      <item>
        <title>Network decomposition 4&amp;#58; weak and strong decompositions</title>
        <description>&lt;p&gt;This is the fourth post of a series on distributed network decomposition. 
The introductory post of this series is 
&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-0&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post, we take a look at the different versions of network decomposition
(weak and strong), and how we can go from one to the other.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/caravane-5.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;recap-of-previous-episodes&quot;&gt;Recap of previous episodes&lt;/h3&gt;

&lt;p&gt;In the previous posts, we saw what a network decomposition is, why it is useful
to the design of local algorithms and how we can build such object with a 
sequential algorithm. Before we describe the distributed efficient algorithm 
(that is the core of series), let us give a further look at two different types
of decompositions.&lt;/p&gt;

&lt;h2 id=&quot;what-is-the-diameter-of-a-component&quot;&gt;What is the diameter of a component?&lt;/h2&gt;

&lt;p&gt;In the &lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-2-impact&quot;&gt;second post&lt;/a&gt; 
of this series we defined network decompositions, saying that the connected 
components of each color class have diameter at most $d$. 
As usual, the diameter is the maximum over all pairs of nodes of the minimum length
of a path between these two nodes. But which edges can you use for these paths: 
only the edges of the subgraph, or all the edges of the graph? 
The first way defines the &lt;em&gt;strong diameter&lt;/em&gt; and the second is the &lt;em&gt;weak diameter&lt;/em&gt;.
These can be very different, as pictured below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/strong-weak.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
On this graph, the blue component has weak diameter 2 and strong diameter 9.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The distributed algorithm we’ll study builds a network decomposition with a 
logarithmic &lt;em&gt;weak diameter&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;going-from-strong-to-weak-diameter&quot;&gt;Going from strong to weak diameter&lt;/h2&gt;

&lt;p&gt;For some applications it might be better to have a strong decomposition, and the 
algorithm provides only a weak one. This is not a big problem. 
The paper of Rozhon and Ghaffari sketches an approach that builds a strong 
decomposition with $O(\log(n))$ colors from a weak one with also $O(\log(n))$ 
colors. 	
It is even simpler to have such a transformation if we just insist on having 
polylogs: 
take your weak decomposition, make the nodes of each component act like in a 
centralized algorithm (by gathering the whole topology of the component and 
simulating), and make them compute a strong decomposition of the component 
itself. For example take a component of weak diameter with color yellow, the nodes 
of this component a strong network decomposition of this component, in some sense
the nodes of this component now have $\log(n)$ shades of yellow. In total this 
makes a strong decomposition with $O(\log^2n)$ colors.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/refined-decompo-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/refined-decompo-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/refined-decompo-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
The first picture is a weak decomposition with two colors, yellow and blue. 
Note that the strong diameter is large in comparison with the weak diameter. Now,
if the yellow component &quot;strongly decompose&quot; itself, we get three shades of 
yellow, and the new yellow components have small strong diameter. We can do this 
for all components of all colors classes, in parallel. 
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;We finish this post with some technical aspects of weak decompositions.&lt;/p&gt;

&lt;h2 id=&quot;a-tree-to-keep-track-of-the-diameter&quot;&gt;A tree to keep track of the diameter&lt;/h2&gt;

&lt;p&gt;Remember that for the centralized construction, we create the component by making 
the balls grow layers by layers. Now suppose that we would build a spanning tree
of the component as we go: at each new layer, we would wire the nodes as leafs 
of the tree. The depth of this tree is between half the diameter and the 
diameter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/boule-arbre-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
The nodes of a component, added layers by layers, during the growth of a ball, 
for the centralized construction. The orange edges are the edges of the tree, 
the gray edges are other edges of the component. The diameter is 5, and the depth of 
the tree is 4. 
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;In the distributed network decomposition, some balls will grow in a similar 
fashion as in the centralized algorithm, but then they will be modified. In 
particular some nodes will leave the component. Also, as we want the tree to have a 
depth that bounds the weak diameter, this tree may have to use edges 
that are not within the component. Hence we use a Steiner tree using the edges 
and nodes that may not be in the component.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/boule-arbre-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
The blue patches show the balls of various distance from the root, using the 
nodes and edges of all the graph. In particular the yellow nodes are not part of 
the component, but they are used by the tree.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;giving-up-the-connectivity&quot;&gt;Giving up the connectivity&lt;/h2&gt;

&lt;p&gt;Actually now that we measure the diameter via the tree, we can consider 
non-connected components.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/boule-arbre-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This change in the definition useful in the distributed algorithm as we don’t 
want to keep track of whether the component is connected or not.&lt;/p&gt;

</description>
        <pubDate>Tue, 05 May 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///network-decomposition-4-weak-strong</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///network-decomposition-4-weak-strong</guid>
      </item>
    
      <item>
        <title>About online conferences</title>
        <description>&lt;p&gt;COVID 19 is forcing a lot of conferences to go online. A few notes about this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/yucca.JPG&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;report-from-the-online-conference-wagon&quot;&gt;Report from the online conference WAGON&lt;/h2&gt;

&lt;p&gt;There is a maths conference named WAGS that took place virtually this year, and
was renamed WAGON for the occasion. One of the organizers wrote report about it 
&lt;a href=&quot;https://www.daniellitt.com/blog/2020/4/20/wagon-lessons-learned&quot;&gt;here&lt;/a&gt;.
It explains what worked and what didn’t.&lt;/p&gt;

&lt;p&gt;They used zoom, with a license purchased by a university, which worked well even 
though they had ~1000 participants (but only a few had video). 
During the talks, there was a chat channel for all participants. 
There was a few moderators (session chairs and specialists), that would answer 
some of the 
questions, and interrupt the speaker only if needed. The speaker did not need to 
look at the chat. Seemingly the chat was very active and very useful. They also 
had some other ways to ask questions, such as the “raise your hand” button of 
zoom, but it seems that the chat was the main channel for this.&lt;/p&gt;

&lt;p&gt;There was also some virtual discussion tables for coffee breaks. 
This was successful up to some 
extent but it seems that it needs some more iterations to find a good mix. 
There are questions like: do you register to get on a table? Do you know the 
names of the people on the table? etc.&lt;/p&gt;

&lt;h2 id=&quot;virtual-conference-schedule&quot;&gt;Virtual conference schedule&lt;/h2&gt;

&lt;p&gt;Conferences going online was a topic before the COVID 19, because of climate 
change and the impact of flying. We are kind of forced to use it now, and that’s 
a test that might show that at least some conferences can be virtual. 
For the moment it seems that the choice for these virtual conferences is 
to make the same event as before 
but online, in the sense that the schedules are similar, that there are coffee 
breaks between talks etc. A question is whether this is the best format. Of 
course for this year, it’s probably the best because it’s what was planned. 
But if some 
conferences stay online, maybe another format would be better.&lt;/p&gt;

&lt;p&gt;In particular, I have seen comments saying that attending 7 hours of online talks 
is even more exhausting than 7 hours of real talks, and I can imagine that. 
Also I’m not sure whether having lunch on your desk with zoom open is great. 
But still you want people to “attend” the conference and do not just come back
and forth between their usual activity and the conference, both because it is 
more efficient this way, and also because if one can go out of the conference 
easily then people will be asked to do it (by their colleagues, by the 
department etc.) although it’s not what they would prefer.&lt;/p&gt;

&lt;p&gt;Maybe a good thing is to gather a few people interested in the conference (from 
your department or city) and to live the conference together. This is probably a 
much nicer experience (for post-pandemic times).&lt;/p&gt;

&lt;h2 id=&quot;impact-of-video-on-climate-change&quot;&gt;Impact of video on climate change&lt;/h2&gt;

&lt;p&gt;Of course virtual conferences have less impact on climate than flying across an 
ocean, but what is the impact of it anyway? 
There is a study by a (serious) French think tank about the impact of digital 
technologies on the climate crisis, and in particular the impact of video. 
You can find summary
&lt;a href=&quot;https://theshiftproject.org/wp-content/uploads/2019/07/Excutive-Summary_EN_The-unsustainable-use-of-online-video.pdf&quot;&gt;here&lt;/a&gt; 
and the full report &lt;a href=&quot;https://theshiftproject.org/wp-content/uploads/2019/07/2019-02.pdf&quot;&gt;there&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A few numbers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The energy consumption related to digital technologies is distributed the 
following way: 45% for production of the devices, 19% for data centers, 16% for 
network, and 20% for terminal use.&lt;/li&gt;
  &lt;li&gt;Digital technologies represent 4% of the greenhouse gases emissions (to be compared
to the 2% due to aviation) and will most probably reach 8% soon (which is the 
percentage of car emissions).&lt;/li&gt;
  &lt;li&gt;Out of these 4%, 1% is due to video.&lt;/li&gt;
  &lt;li&gt;80% of the global data flow is video, and this 80% can is then divided into 
two categories: 60% of streaming-like usage
(youtube, netflix, social media etc.) and 20% of visioconferences-like
usage.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to measure the impact of our Internet activity, the report 
mentions the firefox add-on 
&lt;a href=&quot;https://addons.mozilla.org/en-US/firefox/addon/carbonalyser/&quot;&gt;carbonalyser&lt;/a&gt;	.&lt;/p&gt;

&lt;p&gt;By the way, there is a manifesto for taking climate change into account in TCS 
academic life. It is called &lt;a href=&quot;https://tcs4f.org/&quot;&gt;TCS4F&lt;/a&gt; (TCS for future). It has 
been signed by ~100 people and some organizations such as the conference STACS.&lt;/p&gt;

&lt;h2 id=&quot;another-relation-between-conferences-and-co2&quot;&gt;Another relation between conferences and CO2&lt;/h2&gt;

&lt;p&gt;Another, more unexpected relation between CO2 and conferences: during talks, 
the CO2 level and the temperature in the room increase in a non-negligible way, 
which can explain why it’s difficult to stay awake sometimes.&lt;/p&gt;

&lt;p&gt;A full (quite old) twitter thread on this &lt;a href=&quot;https://twitter.com/battersbot/status/1135926829813063680&quot;&gt;here&lt;/a&gt;.
Also, a &lt;a href=&quot;https://www.nytimes.com/2019/05/06/health/conference-room-air.html&quot;&gt;New York Times article&lt;/a&gt; about this.&lt;/p&gt;

</description>
        <pubDate>Wed, 29 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///online-conference</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///online-conference</guid>
      </item>
    
      <item>
        <title>About journal fees</title>
        <description>&lt;p&gt;Some notes about journal fees. A random walk on the Internet, more than a 
structured essay.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/feuille.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;open-access-and-apc&quot;&gt;Open access and APC&lt;/h2&gt;

&lt;p&gt;As advised by a friend, I decided to submit my 
&lt;a href=&quot;https://www.dii.uchile.cl/~feuilloley/publications/introduction-certification.html&quot;&gt;&lt;em&gt;Introduction to local certification&lt;/em&gt;&lt;/a&gt;
to a journal. 
And as I was the only author involved, 
I thought it was a good occasion to submit the paper to an open-access journal.&lt;/p&gt;

&lt;p&gt;Following some advice, I checked the editors list of the journal 
&lt;a href=&quot;https://www.mdpi.com/journal/algorithms&quot;&gt;&lt;em&gt;Algorithms&lt;/em&gt;&lt;/a&gt; which looks very good.
But then I realized that I had to pay something to get the paper published: the 
article processing charges (APC) (I don’t want to ask money for that from my 
institution). I looked it up, and saw the price: 1000 Swiss francs, that is, 
around 950 euros.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; I was very surprised that it would be so expensive. 
At the end I did two things: submitting the paper to 
&lt;a href=&quot;https://dmtcs.episciences.org/&quot;&gt;&lt;em&gt;DMTCS&lt;/em&gt;&lt;/a&gt; instead, and reading on the 
Internet to learn a bit about these APC.&lt;/p&gt;

&lt;h2 id=&quot;journal-systems&quot;&gt;Journal systems&lt;/h2&gt;

&lt;p&gt;So first there are several business models for scientific journals. The most common
one is based on requesting money from the reader (or the institution of the 
reader). Then there are open-access 
journals where the reader does not pay. Often, in this second system, the 
authors pay. Some journals propose the two schemes. Yet another type of journal 
is free for readers and authors, and can be supported by some institution.&lt;/p&gt;

&lt;p&gt;In this post I’ll focus on the open access systems: the ones where the
reader does not pay.&lt;/p&gt;

&lt;h2 id=&quot;journal-fees&quot;&gt;Journal fees&lt;/h2&gt;

&lt;p&gt;So a first data point is this 1000 CH for &lt;em&gt;Algorithms&lt;/em&gt;. I also remember 
something around 1000 euros for an open-access option of a not full-open-access 
journal. Finally I checked &lt;em&gt;PLOS&lt;/em&gt;, a group of open-access journals, mostly in 
biology, and again it varies between 1000 and 3000 euros.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
So it looks like a price tag around 1000 euros is common.&lt;/p&gt;

&lt;p&gt;What does it pay for? Here is a list of services I can see the publisher offers, 
that are not offered by scholars (reviews are “for free” for example):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Publication: archiving the paper in a safe place, easy to reach.&lt;/li&gt;
  &lt;li&gt;Indexation: registering the paper in the databases, getting a doi.&lt;/li&gt;
  &lt;li&gt;Advertising: maybe advertising the paper, and most probably advertising the 
journal to ensure its prestige.&lt;/li&gt;
  &lt;li&gt;Review process: running the plateforme to receive and store manuscripts, send 
automatic notifications etc.&lt;/li&gt;
  &lt;li&gt;Editorial management: the main editors might be paid by the publisher.&lt;/li&gt;
  &lt;li&gt;Type-setting: improving the visual aspects of the papers.&lt;/li&gt;
  &lt;li&gt;Language: in some fields, the journal improves the writing.&lt;/li&gt;
  &lt;li&gt;Printing: for non-electronic journals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s difficult to measure how much all this costs, so I’ll compare with another 
plateforme: LIPIcs.&lt;/p&gt;

&lt;h2 id=&quot;lipics-apc&quot;&gt;LIPIcs APC&lt;/h2&gt;

&lt;p&gt;LIPIcs handles proceedings of computer science conferences. Basically the 
conference pays LIPIcs for this job, and the conference itself gets the money 
from the authors via the registration fees. 
The APC is 60 euros for a 20 pages paper.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;An interesting document is 
&lt;a href=&quot;https://www.dagstuhl.de/fileadmin/dagpub/apc/seidel-apc-increase-letter-march2016.pdf&quot;&gt;a letter&lt;/a&gt; 
telling the story of this APC. 
In a nutshell, the first APC 15 euros. This was an estimate of the cost and more 
and it included the fact that the journal was helped by
Dagstuhl’s funds. But this was not ok with EU market laws, as it was unfair to 
the classic publishers. They had to increase the price from 15 to 60 euros, to
fill the gap of the Dagstuhl’s funds, and also because the 15 euros estimate 
was a bit too low in comparison with the work needed. More precisely, they 
didn’t expect that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;many authors and committees would work less than required on the type-setting, 
which meant more polishing from their side&lt;/li&gt;
  &lt;li&gt;people would use so many weird latex packages that they had to work a lot to 
solve the compatibility issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(Thanks to a donation, they could change the price progressively from 15 to 60.)&lt;/p&gt;

&lt;h2 id=&quot;comparison-with-lipics&quot;&gt;Comparison with LIPIcs&lt;/h2&gt;

&lt;p&gt;So, why is there such a difference between LIPIcs and an open-access journal? 
I see several answers, beyond “journals want to make money”. Note that the 
quality is not a difference, LIPICs proceedings look great to me. Also note that 
PLOS is a non-profit organization, so is supposed to not make so much money.&lt;/p&gt;

&lt;h3 id=&quot;conference-proceedings-versus-journal&quot;&gt;Conference proceedings versus journal&lt;/h3&gt;
&lt;p&gt;The fact that LIPIcs handles only conference proceedings and is not a journal
could change several aspects.
Conference means a lighter review process (usually only one round). 
Also it means that the number of 
published papers is very controlled: you know it in advance basically. Also journal
papers are usually longer, and LIPIcs charges 60 euros for 20 pages, 
so 100 pages means 300 euros instead of 60. 
Something interesting here is that in the LIPICs letter cited above, they say 
that gold-open-access proceedings APC is usually in the range of 200 
to 600 euros, which is less than what I sampled. This might be because of this
conference versus journal topic.&lt;/p&gt;

&lt;h3 id=&quot;less-type-setting-work&quot;&gt;Less type-setting work&lt;/h3&gt;

&lt;p&gt;LIPICs expects authors and conference committees to produce papers that need
almost no additional work before being published. For a conference, this work, 
if not done by the authors, could require days of work from a member of the 
committee (I remember that Jukka Suomela worked a lot on DISC proceedings some 
years ago).&lt;/p&gt;

&lt;h3 id=&quot;small-scale&quot;&gt;Small scale&lt;/h3&gt;

&lt;p&gt;LIPICs does not handle hundreds of papers per day like some journals do (although 
not in TCS probably). Maybe the fact that it has a small scale allows the 
editors to be professors in some university, and not people paid by the publisher 
to run the journal.&lt;/p&gt;

&lt;h3 id=&quot;some-institutional-support&quot;&gt;Some institutional support&lt;/h3&gt;

&lt;p&gt;Maybe LIPICs has some support that does not come directly as money. For example 
it seems that they have a partnership with the German National Library for 
archiving the papers.&lt;/p&gt;

&lt;h3 id=&quot;altogether&quot;&gt;Altogether&lt;/h3&gt;

&lt;p&gt;Altogether it looks like there shouldn’t be such a difference in prices, in my 
opinion, but there are probably many aspects that I haven’t taken into account. 
Anyway, many thanks are due to the LIPIcs team for their great job.&lt;/p&gt;

&lt;h2 id=&quot;fair-open-access&quot;&gt;Fair open-access&lt;/h2&gt;

&lt;p&gt;As said above I ended up submitting the paper to DMTCS, which a journal that
is free for both readers and authors. This seems to be called &lt;em&gt;fair open access&lt;/em&gt;.
Here the model is basically that the journal is run by scholars, that may pay a 
professional publisher for services such as printing. The money comes from 
institutions, as general donations, not fees related to specific papers. And the
money given to professional publishers should be controlled rigorously. 
See the 
&lt;a href=&quot;https://www.fairopenaccess.org/the-fair-open-access-principles/&quot;&gt;fair open access principles&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;An interesting bit in the explicative notes of these principles, is that they 
fix the maximum fee per paper a publisher can ask to the journal to 1000 euros 
(and say that “substantially lower fees should be possible in many case”).&lt;/p&gt;

&lt;p&gt;In the case of DMTCS, the two large scientific French public institutions, CNRS 
and INRIA, sponsor the plateforme &lt;a href=&quot;https://www.episciences.org/&quot;&gt;Episciences&lt;/a&gt;, 
that the journal uses. Episciences supports around 15 fair open-access journals.&lt;/p&gt;

&lt;p&gt;The list of fair open-access journals can be found on the 
&lt;a href=&quot;https://freejournals.org/&quot;&gt;website of the Free Journal Network&lt;/a&gt;. Such journals
in TCS are: &lt;a href=&quot;https://dmtcs.episciences.org/&quot;&gt;DMTCS&lt;/a&gt;, &lt;a href=&quot;jocg.org&quot;&gt;JOCG&lt;/a&gt;, 
&lt;a href=&quot;jgaa.info&quot;&gt;JGAA&lt;/a&gt;, &lt;a href=&quot;https://lmcs.episciences.org&quot;&gt;LMCS&lt;/a&gt;, 
&lt;a href=&quot;https://smai-jcm.math.cnrs.fr/index.php/SMAI-JCM/&quot;&gt;SMAI JCM&lt;/a&gt;,
&lt;a href=&quot;https://digitalcommons.georgiasouthern.edu/tag/&quot;&gt;TAG&lt;/a&gt; ands
&lt;a href=&quot;https://theoryofcomputing.org/&quot;&gt;TOC&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And there several more if you extend the scope to combinatorics.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.mdpi.com/journal/algorithms/apc&quot;&gt;APC page of &lt;em&gt;Algorithms&lt;/em&gt;&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://plos.org/publish/fees/&quot;&gt;Publication fees for PLOS journals&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.dagstuhl.de/en/publications/lipics/processing-charge/&quot;&gt;LIPIcs Processing charge&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 21 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///journal-fees</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///journal-fees</guid>
      </item>
    
      <item>
        <title>Network decomposition 3&amp;#58; Centralized construction</title>
        <description>&lt;p&gt;This is the third post of a series on distributed network decomposition. 
The introductory post of this series is 
&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-0&quot;&gt;here&lt;/a&gt;. 
This post explains how to build a network decomposition in a centralized manner.
As a consequence it also shows that such decompositions exist with nice 
parameters.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/caravane-4.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;recap-of-previous-episodes&quot;&gt;Recap of previous episodes&lt;/h3&gt;

&lt;p&gt;In the previous posts we saw how network decomposition can be useful to build 
efficient local algorithms. Such algorithms are efficient only if the network 
decomposition has good parameters, that is a small number of colors, and a small
diameter for each component. 
We won’t bother with multiplicative constants, or even constant exponents, 
so let’s say that basically, we look for something like in the following picture, 
with $\log n$ colors, and $\log n$ diameter.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/network-decomposition-log.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;basic-technique-to-get-a-coloring&quot;&gt;Basic technique to get a coloring&lt;/h2&gt;

&lt;p&gt;Suppose you want to compute a coloring in a graph in the most simple way. 
You can start with the first color $c_1$, pick a node $v$, put color $c_1$ on it, 
then choose a second node that is not a neighbor of $v$, put again color $c_1$, 
and so on and so forth,
until you cannot use color $c_1$ anymore. Then you take color $c_2$, and repeat 
the process, considering only the nodes that do not have color $c_1$, etc.&lt;/p&gt;

&lt;p&gt;This is basically repeating an algorithm for MIS we mentionned in the 
&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-1-local-algorithms&quot;&gt;post about local algorithms&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-0.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-4.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-5.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
Step by step construction of a coloring. (1) The graph, (2) the result of the 
computation of the first color class (exactly as in the MIS algorithm), with the 
frozen nodes in grey, (3) the remaining nodes and edges after deletion of the 
colored nodes, (4) second color, (5) third color, and (6) the computed coloring.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;adapting-the-technique-to-network-decomposition&quot;&gt;Adapting the technique to network decomposition&lt;/h2&gt;

&lt;p&gt;We adapt the technique from the previous section to network decomposition. 
That is, instead of picking a 
node and coloring it, we pick a node, and color the ball of radius $\log n$ 
around it. Similarly to what happens in the coloring algorithm above, 
every time we color a ball, we freeze all the nodes that are adjacent to it. 
That is, they will not be considered any more for the color at hand. Once we are 
finished with a color, we go to the next color, remove the nodes of the previous 
colors, defreeze the frozen nodes, and start again.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-log-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-log-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-log-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
Computation of the first color class: (1) choose a node, and select all the 
nodes at distance at most $\log n$ from it, (2) freeze all the nodes that are 
adjacent to a selected node, and (3) repeat the operation until all nodes are
either selected or frozen.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-log-4.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-log-5.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-log-6.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
Computation of the second color class: (1) and (2) remove all the 
nodes of the first color class, and defreeze all frozen nodes. (3) Run the same 
procedure as for the first color class. 
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-log-7.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-log-8.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/coloring-log-9.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
Computation of the third color class: again, (1) and (2) remove all the 
nodes of the previous color classes, and defreeze all frozen nodes. (3) Run the 
same procedure as for the first color classes. Now all nodes are selected, thus 
the algorithm stops.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/coloring-log-10.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;small&gt;&lt;i&gt;
The network decomposition produced by the algorithm.
&lt;/i&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;an-unbalanced-coloring&quot;&gt;An unbalanced coloring&lt;/h2&gt;

&lt;p&gt;The first thing one can note on the picture above is that the decomposition is 
not balanced at all: a huge part of the network is red, a small part is yellow, 
and a tiny part is blue. That might look like a bad thing, as for algorithms 
having balanced decomposition is usually a nice feature. But actually this is not a 
problem for the algorithms built on a network decomposition, as presented in 
the &lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-2-impact&quot;&gt;previous post&lt;/a&gt;. 
And, in fact this imbalance is good for us. Indeed suppose that each color 
covers at least half of the remaining nodes, then after at most $\log n$ steps, 
all nodes are colored. This, in turns, implies that the number of colors is 
logarithmically bounded, which is what we were looking for.&lt;/p&gt;

&lt;p&gt;But, is this always the case? No, you can have a situation like the one on the 
following picture: you pick a node and in its $\log n $ neighborhood, there are 
very few nodes, but the number of nodes that is frozen at the border is huge. 
If this happens everywhere, the number of nodes colored by a the first color 
class is not a constant fraction of the total number of nodes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/blow-up.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Therefore, a bad situation for us is when the ratio between what is inside 
the ball, and its border is small.&lt;/p&gt;

&lt;p&gt;We could try to show that we can choose the centers in a smart way to avoid this 
behavior, but it wouldn’t help much with the distributed construction that 
follows, as such smart choice could require a large view of the graph.
Instead we control the size of the balls.&lt;/p&gt;

&lt;h2 id=&quot;growing-a-ball&quot;&gt;Growing a ball&lt;/h2&gt;

&lt;p&gt;A technique to ensure that the interior/border ratio is large is to build the 
ball around the selected center in the following way, called &lt;em&gt;ball carving&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start with only the center&lt;/li&gt;
  &lt;li&gt;Iterate the following:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Measure the number of nodes in the border&lt;/li&gt;
  &lt;li&gt;If it is at least twice the number of nodes in the ball, take all these nodes 
in the ball, and restart the loop.&lt;/li&gt;
  &lt;li&gt;Otherwise leave the loop.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Freeze all the nodes of the border.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the loop can be run for at most $\log n$ iterations, as otherwise 
the ball would contain more than $n$ nodes. Thus the balls built this way are ok
with the definition of the network decomposition.&lt;/p&gt;

&lt;p&gt;By construction, the size of the border of the ball is at most the size of the
interior of the ball. That is, if we select $m$ nodes to be in the ball, we 
freeze at most $m$ new nodes. This is enough for the whole construction to work.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Next post of the series: &lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-4-weak-strong&quot;&gt;Weak and strong decomposition&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 20 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///network-decomposition-3-centralized</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///network-decomposition-3-centralized</guid>
      </item>
    
      <item>
        <title>Online talks</title>
        <description>&lt;p&gt;Many things have gone online, including talks and conferences. A small post with
pointers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/plume.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;online-seminars&quot;&gt;Online seminars&lt;/h2&gt;

&lt;p&gt;Many seminars have moved to online settings.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Terence Tao has a 
&lt;a href=&quot;https://terrytao.wordpress.com/2020/04/07/mathematics-seminars-list/&quot;&gt;list of list of online maths seminars&lt;/a&gt;, 
including &lt;a href=&quot;http://math.mit.edu/~aosun/online_seminars.html&quot;&gt;Online maths seminars&lt;/a&gt; 
that has more than 15 talks per day (in all kinds of mathematics).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The same thing but speciafically for TCS can be found 
&lt;a href=&quot;https://cstheorytalks.wordpress.com/&quot;&gt;here&lt;/a&gt;. (One or two talks per day.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Aalto university also has &lt;a href=&quot;https://users.aalto.fi/~uittoj3/seminar.html&quot;&gt;its seminar&lt;/a&gt; 
(that is not listed (yet?) in the lists above).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The online TCS seminar, &lt;a href=&quot;https://sites.google.com/site/plustcs/&quot;&gt;TCS+&lt;/a&gt; has 
increased its frequency from one per month to one per week (and all the previous 
talks are available).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Simons Institute also has a lot of good videos, see 
&lt;a href=&quot;https://simons.berkeley.edu/videos&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;online-courses&quot;&gt;Online courses&lt;/h2&gt;

&lt;p&gt;Many universities have MOOCs, or at least videos of some classes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Jukka Suomela has just started a new iteration of his parallel programming 
class, and this year of course everything is online. The videos are 
&lt;a href=&quot;https://www.youtube.com/playlist?list=PL2RY7P3JxZN9Eeu2-XUbD4E4N3UNL6PV8&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MIT has, for example, videos lectures 
&lt;a href=&quot;https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-890-algorithmic-lower-bounds-fun-with-hardness-proofs-fall-2014/&quot;&gt;about lower bounds&lt;/a&gt;,
and &lt;a href=&quot;https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/&quot;&gt;advanced data structures&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;online-conferences&quot;&gt;Online conferences&lt;/h2&gt;

&lt;p&gt;Conferences that were suppose to take place this spring or this summer decide one
by one to held virtually. This is the case of &lt;a href=&quot;https://icalp2020.saarland-informatics-campus.de/&quot;&gt;ICALP&lt;/a&gt;
and &lt;a href=&quot;http://acm-stoc.org/stoc2020/&quot;&gt;STOC&lt;/a&gt; for example.&lt;/p&gt;

&lt;p&gt;The ACM issued a guide on how to organize such online conferences. The pdf is 
&lt;a href=&quot;https://people.clarkson.edu/~jmatthew/acm/VirtualConferences_GuideToBestPractices_CURRENT.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Tue, 14 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///online-talks</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///online-talks</guid>
      </item>
    
      <item>
        <title>Network decomposition 2&amp;#58; Impact on distributed algorithms</title>
        <description>&lt;p&gt;This is the second post of a series on distributed network decomposition. 
The introductory post of this series is 
&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-0&quot;&gt;here&lt;/a&gt;. 
This post explains why network decomposition is useful.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/caravane-2.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;recap-of-previous-episodes&quot;&gt;Recap of previous episodes&lt;/h3&gt;

&lt;p&gt;We saw in the 
&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-1-local-algorithms&quot;&gt;first post&lt;/a&gt; 
of the series that one can simulate the greedy centralized algorithm in the local 
model, by using the identifiers as a schedule, but that this was horribly slow.
We also looked at an algorithm that gather the whole topology of the graph, this
will be useful here.&lt;/p&gt;

&lt;h2 id=&quot;everything-is-easier-with-a-coloring&quot;&gt;Everything is easier with a coloring&lt;/h2&gt;

&lt;p&gt;It is easy to see how we could use less time steps to run the greedy algorithm: 
if you have two nodes that are far enough one from the other, you can select 
both without running into trouble. And then instead of two, you can select a
large number of nodes, as long as they are far away one from the other. 
This way in one round you can select and de-select a lot of nodes.&lt;/p&gt;

&lt;p&gt;Now, more generally, imagine that a mysterious friend gives you a (proper) 
$k$-coloring of the graph. You can use it to parallelize the algorithm the 
following way. First, pick the first color, select all of its nodes, de-select 
all the nodes adjacent to these nodes. This cannot create a problem as the nodes
of a color class cannot be adjacent. Then continue by doing the same with the 
nodes of the second color that are still active. And so on and so 
forth, until you have considered all the colors.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-coloring-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-coloring-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-coloring-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-coloring-4.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This algorithm basically takes $k$ times steps, which is awesome if your mysterious 
friend gave you a 10-coloring.&lt;/p&gt;

&lt;p&gt;Now there are two problems:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;You do not have a mysterious friend, and computing a good coloring yourself is 
essentially as hard as computing the MIS.&lt;/li&gt;
  &lt;li&gt;The graph might not be colorable with a small number of colors: maybe it’s
chromatic number is $n/2$, and then the algorithm takes $n/2$ rounds, which is 
not much better than before.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;network-decomposition-to-the-rescue&quot;&gt;Network decomposition to the rescue&lt;/h2&gt;

&lt;p&gt;The idea of network decomposition is to relax the strong constraints of a 
coloring into something more handy, while keeping the idea of a schedule of the 
type:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;All nodes of group 1 do something and all the others are passive. At the end 
of this phase, they all have an output.&lt;/li&gt;
  &lt;li&gt;All nodes of group 2 do something and all the others are passive. At the end 
of this phase, they all have an output.&lt;/li&gt;
  &lt;li&gt;Etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In a nework decomposition, instead of having a coloring “at the level of the nodes” that 
is where every node has a color different from its neighbors, we will color 
clusters of nodes and have a coloring “at the level of the clusters”. In some 
sense we change scale. A network decomposition looks a bit like this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/impact-decompo-6.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now there are three questions: What is exactly a network decomposition? 
How to use it? and How to compute it?&lt;/p&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Definition&lt;/em&gt; A network decomposition with parameters $c$ and $d$ is a labeling of 
the nodes with colors from 1 to $c$, such that for any given color, the (maximal)
connected components with this color have diameter at most $d$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/impact-decompo-5.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[There are some subtleties with the definition, but let’s wait a bit before 
looking into that.]&lt;/p&gt;

&lt;p&gt;So for example a proper $k$-coloring is a network decomposition with parameter 
$d=1$ and $c=k$. Also it is easy to have a network decomposition with parameters
$d=n$ and $c=1$: color all nodes with color 1. 
In all this series of posts, what we will look for is a decomposition where both 
parameters are (poly)logarithmic in $n$.&lt;/p&gt;

&lt;h2 id=&quot;how-to-use-it&quot;&gt;How to use it?&lt;/h2&gt;

&lt;p&gt;We cannot use the exact same strategy as when we had a proper coloring. Indeed, 
in a network decomposition several nodes of the same color can be adjacent, thus 
they cannot just wake up and select themselves. 
Now it’s useful to remember the last part of the previous post about a general
algorithm to solve any problem in time $O(n)$. 
Basically this algorithm was doing the following: at any point in time, every 
node sends to its neighbors everything it knows about the graph so far. Then
steps by steps we have the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;When it starts a node knows only its identifier, and it sends it to its 
neighbors.&lt;/li&gt;
  &lt;li&gt;Then it receives the identifiers of its neighbors, thus in some sense it knows 
the graph at distance one around itself: it knows its degree and the IDs of its 
neighbors.&lt;/li&gt;
  &lt;li&gt;Then the node sends this information to its neighbors, and it recives the 
analogue information from its neighbors. With this, it can reconstruct its 
neighborhood at distance 2. That is, it is exactly the same as if it could see 
at distance 2 in the graph.&lt;/li&gt;
  &lt;li&gt;Iterating this process, after $k$ steps all nodes know their neighborhoods 
at distance $k$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the proof of the general $O(n)$ algorithm, we iterated this process until 
every node could see all the graph. That is we waited for $D$ steps, where $D$ 
is the diameter of the graph (and then as $D \leq n$, we got the result). But this 
process can be useful even if we do not wait up to $D$ rounds, that is, even if 
the nodes do not see the whole graph.&lt;/p&gt;

&lt;p&gt;Given the network decomposition, let’s start by activating all the nodes of 
color 1. Now use the protocol above, for $d$ rounds. After this time, a node of
color 1 knows all the nodes of its connected component. Indeed it can see at 
distance $d$ and we have been promised that the diameter of each connected 
component is at most $d$. Once this is done, we ask each node of the connected 
component to forget about the rest of the graph, to compute an MIS in what 
remains, and to output whether it is selected or not in this MIS. Note that it 
is exactly the same thing as what we did in the previous post but on a subgraph.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/impact-decompo-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/impact-decompo-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;At the end of the phase, we have only selected and non-selected nodes in all the 
connected components of the first color. Note that these local MIS are correct, 
in the sense that there is no conflict: two connected components are necessarily 
non-adjacent (otherwise they wouldn’t be maximal). To finish this phase with the 
first color class, we de-select the nodes of the other color classes that are 
adjacent to a newly selected node of the first color class. After this step, all
these nodes will not participate in the computation, and we can imagine that 
they are removed from the graph&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/impact-decompo-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/impact-decompo-4.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We can iterate this for all the color classes, just as we did when we had a 
proper coloring. Let us check the complexity. For each color we need the view at 
distance $d$, which takes $d$ rounds, and there are $c$ colors, so we have an 
algorithm in basically $d\times c$ rounds. As we look for a network decomposition 
with polylogarithmic parameters, this gives us a polylogarithmic algorithm, 
which is awesome!&lt;/p&gt;

&lt;p&gt;But now we need to be able to build the network decomposition fast. This is the 
core topic of this series of posts.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Next post of the series: &lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-3-centralized&quot;&gt;Centralized construction&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 10 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///network-decomposition-2-impact</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///network-decomposition-2-impact</guid>
      </item>
    
      <item>
        <title>Network decomposition 1&amp;#58; Local algorithms</title>
        <description>&lt;p&gt;This the first real post of a series on distributed network decomposition. 
The introductory post of this series is 
&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-0&quot;&gt;here&lt;/a&gt;. 
This post is a quick introduction to local algorithms, with the example of the
maximal independent set problem. If you have heard about the local model before, 
you probably know everything in this post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/caravane-1.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-problem58-computing-a-maximal-independent-set&quot;&gt;A problem: computing a maximal independent set&lt;/h2&gt;
&lt;p&gt;A typical problem of network distributed computing is computing a 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximal_independent_set&quot;&gt;maximal independent set&lt;/a&gt;
(MIS). An MIS is a set $S$ of nodes of the graph such that not two nodes of $S$
are adjacent, and for every node not in $S$, there is neighbor in $S$.&lt;/p&gt;

&lt;p&gt;The two pictures below &lt;em&gt;do not&lt;/em&gt; represent an MIS: the first one 
because of two adjacent selected nodes, and the second because of an “isolated node”.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-arete.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-noeud.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The two following pictures represent MISs. Note that one in &lt;em&gt;maximum&lt;/em&gt; (it has 3 
nodes, and no MIS on 4 nodes exists), but the other is just maximal, and it’s 
also fine.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-maxi.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt; &lt;img src=&quot;assets/MIS-pas-maxi.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;easy-to-solve-in-a-centralized-manner&quot;&gt;Easy to solve in a centralized manner&lt;/h2&gt;

&lt;p&gt;It’s very easy to solve this problem in a centralized manner. 
An algorithm is the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Label all nodes as &lt;em&gt;active&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;As long as it is possible:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Take an arbitrary active node&lt;/li&gt;
  &lt;li&gt;Put it in the MIS&lt;/li&gt;
  &lt;li&gt;Label this node and all its neighbors as &lt;em&gt;inactive&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-seq-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-seq-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/MIS-seq-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A first problem for us, with this algorithm, is that it is not distributed: you
need an external entity to chose the “arbitrary active node”. This is some kind 
of scheduler, who decides which node is “doing something” at any step.&lt;/p&gt;

&lt;h2 id=&quot;using-identifiers-to-simulate-a-centralized-scheduler&quot;&gt;Using identifiers to simulate a centralized scheduler&lt;/h2&gt;

&lt;p&gt;In our model, we will assume that every node has a unique identity. This 
identifier is a number in $[1,n^2]$, where $n$ is the size of the netork 
(in general we take a large enough 
polynomial for the upper bound, but for concreteness let’s say $n^2$). 
For a node $v$, let $ID(v)$ be its identifier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/MIS-ID.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;65%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using these identifier we will simulate the centralized scheduler.&lt;/p&gt;

&lt;p&gt;All nodes start at the same time, and follow time steps (time-step= 1, 2, 3 etc. ).
They start with a status that is &lt;em&gt;active&lt;/em&gt;.
The following algorithm is run at all nodes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If time-step = $ID(v)$ and status = &lt;em&gt;active&lt;/em&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Change status to &lt;em&gt;selected&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Send message “selected” to all neighbors&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;If the time-step $\neq ID(v)$ and status = &lt;em&gt;active&lt;/em&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Wait for a message “selected” from neighbors&lt;/li&gt;
  &lt;li&gt;If one arrives, change status to &lt;em&gt;not selected&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One can check that on our example, with the identifier given above, the run of 
the algorithm simulates the run of the centralized algorithm. Another identifier 
assignment would correspond to another centralized scheduler, and would give 
another MIS.&lt;/p&gt;

&lt;p&gt;Note that the algorithm is correct because the identifiers are all distinct. 
Indeed, if two neighbors had the same identifier, they would be selected at the 
same time, and the outcome would not be an MIS.&lt;/p&gt;

&lt;p&gt;Now to evaluate the performance of a local algorithm, we measure the number of 
time steps before the solution is completed. Here it is $n^2$ in general, as 
one would have to wait for the node with the largest identifier. 
This is a very poor complexity. Indeed we finish this post with a proof that any
problem can be solved in $O(n)$ time steps.&lt;/p&gt;

&lt;h2 id=&quot;general-algorithm-in-on-steps&quot;&gt;General algorithm in $O(n)$ steps&lt;/h2&gt;

&lt;p&gt;Consider the following algorithm (that is described partially by the pictures 
below).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Send identifier to all neighbors&lt;/li&gt;
  &lt;li&gt;Receive the identifier of all neighbors, and build the list of the adjacent 
edges, e.g. $(ID(v),ID(w))$ for a node $v$ receiving a message from a neighbor 
$w$.&lt;/li&gt;
  &lt;li&gt;Send these edges to all neighbors.&lt;/li&gt;
  &lt;li&gt;For $n$ time steps: send the set of all the edges received so far.&lt;/li&gt;
  &lt;li&gt;Then build a local copy of the graph, solve the problem on this copy, and 
output the part of the solution that correspond to the node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following set of pictures shows how the information about the existence of
the egde (2,5) is built and then broadcasted to the whole graph.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;assets/flooding-1.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt; &lt;/td&gt;
      &lt;td&gt; &lt;img src=&quot;assets/flooding-2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;assets/flooding-3.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;100%&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This algorithm is correct because after $n$ steps of flooding, all nodes know 
about all the edges, thus the local copy of the graph that each node has is 
correct, and then the output of the algorithm is also correct.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Next post of the series: &lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-2-impact&quot;&gt;Impact on distributed algorithms&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///network-decomposition-1-local-algorithms</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///network-decomposition-1-local-algorithms</guid>
      </item>
    
      <item>
        <title>Network decomposition&amp;#58; Introduction</title>
        <description>&lt;p&gt;This post is an introduction (and a table of contents) for a series of posts on
distributed network decomposition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/caravane-0.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the important papers of these recent years in distributed graph 
algorithms will appear at STOC this summer:&lt;/p&gt;

&lt;p&gt;“Polylogarithmic-Time Deterministic Network Decomposition and Distributed 
Derandomization” by &lt;a href=&quot;https://n.ethz.ch/~rozhonv/&quot;&gt;Václav Rozhoň&lt;/a&gt; and 
&lt;a href=&quot;https://people.inf.ethz.ch/gmohsen/&quot;&gt;Mohsen Ghaffari&lt;/a&gt; from ETH Zurich.&lt;/p&gt;

&lt;p&gt;It is basically the description of a distributed algorithm that performs what is 
called a &lt;em&gt;network decomposition&lt;/em&gt;, faster than before. This decomposition can 
then be used to do many other things fast, and the paper solves several important 
open problems of the field.&lt;/p&gt;

&lt;p&gt;I quickly looked at the paper when it appeared on arxiv, but I want to 
understand it better, and writing a series of posts about it is good way to do 
this.&lt;/p&gt;

&lt;p&gt;Here is the my current plan for this series:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-1-local-algorithms&quot;&gt;Local algorithms&lt;/a&gt;&lt;/em&gt;, 
an introduction to local algorithm, with the example of 
the maximal independent set problem.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-2-impact&quot;&gt;Impact for distributed algorithms&lt;/a&gt;&lt;/em&gt;, 
that is, why is a network decomposition so useful.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-3-centralized&quot;&gt;Centralized construction&lt;/a&gt;&lt;/em&gt;, 
how to build a network decomposition with good parameters in a centralized manner.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-4-weak-strong&quot;&gt;Weak and strong decomposition&lt;/a&gt;&lt;/em&gt;, 
some useful precisions about the decomposition&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://discrete-notes.github.io/network-decomposition-5-algorithm&quot;&gt;The algorithm&lt;/a&gt;&lt;/em&gt; of Ghaffari and Rozhon.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 08 Apr 2020 00:00:00 +0200</pubDate>
        <link>https://discrete-notes.github.io///network-decomposition-0</link>
        <guid isPermaLink="true">https://discrete-notes.github.io///network-decomposition-0</guid>
      </item>
    
  </channel>
</rss>
